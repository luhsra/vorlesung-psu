#+TITLE: 09 - Optimierung
#+SETUPFILE: setup.org
#+DATE: [2020-06-23]
#+VIDEO_URL: https://flowcasts.uni-hannover.de/nodes/wDZBj
#+INCLUDE: export-prologue.org
#+PROPERTY: header-args:latex :tangle yes :noweb yes :tangle-macros yes :tangle-prologue topic :tangle-epilogue endtopic :exports none

#+NAME: topic
#+BEGIN_SRC latex :tangle no
\psuSectionStart{{{{property(ITEM)}}}}{{{{n(block)}}}}
#+END_SRC

#+NAME: endtopic
#+BEGIN_SRC latex :tangle no
\psuSectionStop{{{{property(ITEM)}}}}{{{{n(block,-)}}}}
#+END_SRC

* Gedanken zu dieser Vorlesung                                     :noexport:
- Was darf Optimierung machen?
- Block-Lokale Optimierungen
  - Konstantenfaltung
  - Konstanten- und Kopiepropagation
  - Alias-Problematik
- Optimierung des Kontrollflusses
  - Redundante Jumps entfernen
  - Blöcke verschmelzen
- Müll aufräumen
  - Dead-Code Elimination
  - Dead-Store Elimination

* Was leistet die "Optimierung"?
:PROPERTIES:
:CUSTOM_ID: 09-einleitung
:END:

#+begin_src latex
  \subtitle{{{{subtitle()}}}}
  \maketitleframe

  \begin{frame}{Einordnung in die Vorlesung: Optimierung}
    \begin{center}
      \includegraphics[page=7,width=0.9\linewidth]{fig/01-overview-small}
    \end{center}
    \bi
    \ii \structure{Hardwareunabhängige Optimierung} auf dem Zwischencode {
      \bi
      \ii IR-Code wurde \alert{ohne Rücksicht} auf die effiziente Ausführbarkeit erstellt.
      \ii \structure{Effektive Entwickler}: Welche Arbeiten kann er dem Optimierer überlassen?
      \ii \structure{Effiziente Entwickler}: Wie lege ich dem Optimierer keine Steine in den Weg?
      \ei
    }
    \ei
  \end{frame}

  \begin{frame}[t,fragile]{Motivation: Da geht noch was!}
    \vspace{-2em}
    \begin{columns}[t]

    
      \begin{onlyenv}<handout:1-2|1-2>
        \begin{column}{0.49\textwidth}
          \begin{code}[tag=L0]
          \lstinputlisting[style=lzero]{lst/09-example.l0}
        \end{code}
      
        Bei genauer Betrachtung sehen wir, dass dieses Programm \ALERT{immer 0} zurückgeben wird.

        \bigskip
        \uncover<handout:2|2>{Dafür wird echt viel gerechnet!}
      \end{column}

    \end{onlyenv}
    \begin{onlyenv}<handout:3-|3->
      \begin{column}{0.49\textwidth}
        \mbox{}\par\bigskip
        \enquote{Der Übersetzer schaut genau hin}
        \medskip
        \be
        \ii {Konstante Auswertung} {
          \\[0.5ex]\enquote{Bekanntes Vorausberechnen}\\[0.5ex]
          \bi
          \ii<handout:4-|4-> Arithmetische Berechnungen
          \ii<handout:5-|5-> Konstante Sprungbedingungen
          \ei
        }
        \ii<handout:6-|6-> {Konstanten- und Kopiepropagation}
        \ii<handout:7-|7-> Goto-auf-Goto  eliminieren
        \ii<handout:8-|8-> {Dead-Code Elimination}
        \ii<handout:9-|9-> {Basic-Block Merge}\\[2ex]
        \ii<handout:10-|10-> \ldots repeat, until no change.
        \ee
      \end{column}
    \end{onlyenv}
    \begin{column}<handout:2-|2->{0.49\textwidth}\centering
      \only<handout:1-3|1-3>{\btAnimation[height=0.9\textheight,width=\textwidth,keepaspectratio]{raisebox,1:<1->}{fig/09-example-unoptimized}}%
        \btAnimation[height=0.9\textheight,width=\textwidth,keepaspectratio]{raisebox,1:<4>}{fig/09-example-cf1}%
        \btAnimation[height=0.9\textheight,width=\textwidth,keepaspectratio]{raisebox,1:<5>}{fig/09-example-cf2}%
        \btAnimation[height=0.9\textheight,width=\textwidth,keepaspectratio]{raisebox,1:<6>}{fig/09-example-cvp}%
        \btAnimation[height=0.9\textheight,width=\textwidth,keepaspectratio]{raisebox,1:<7>}{fig/09-example-jumps}%
        \btAnimation[height=0.9\textheight,width=\textwidth,keepaspectratio]{raisebox,1:<8>}{fig/09-example-dce}%
        \btAnimation[height=0.9\textheight,width=\textwidth,keepaspectratio]{raisebox,1:<9>}{fig/09-example-merge}%
        \btAnimation[width=0.5\textwidth]                                  {raisebox, 1:<10>}{fig/09-example-repeat}
      \end{column}
    \end{columns}
  \end{frame}
#+end_src

In der letzten Vorlesung haben wir den abstrakten Syntaxbaum zu einem IR-Programm übersetzt.
Dabei haben wir den AST linearisiert und uns auf eine Ausführungsreihenfolge von Befehlen festgelegt.
Bei diesem Prozess der Codeerzeugung haben wir darauf verzichtet, super performanten und effizienten Zwischencode zu erzeugen, der beispielweise minimal viele virtuelle Register verwendet.
Ich habe ganz absichtlich eine Trennung der Belange von Codeerzeugung und Optimierung vorgenommen, um die Prinzipien des einen Bereichs nicht mit Prinzipien des anderen Bereichs zu vermengen.
In dieser Vorlesung wollen wir einen kurzen Blick auf den Bereich der Optimierung werfen.

Dazu will ich als erstes den Umfang aufgrund der Kürze der Zeit gleich einmal einschränken:
Ganz generell ist die Optimierung von Programmen ein breites und altes Feld.
Optimierungen können im Übersetzer in jedem Zwischenschritt durchgeführt werden.
Der Übersetzer kann bereits beim Parsen Ausdrücke, die ein konstantes Ergebnis liefern (~2+3~), durch das Ergebnis (~5~) ersetzen[fn::GCC tut dies, Clang nicht. Daher ist Clang einfacher für Source-to-Source-Transformationen anwendbar.]. Aber auch der Zwischencode oder der bereits erstellte Assembler können optimiert werden. 
So breit können wir dieses Thema einfach nicht besprechen und ich werde mich daher auf einige relativ einfache Optimierungen auf der IR-Ebene konzentrieren. 
Jedoch werden Sie dennoch einen Eindruck und hoffentlich eine gewisse Intuition bekommen, welche Fähigkeiten ein Übersetzer haben *kann* und wie Sie, als Entwickler, diesen Mechanismen möglichst wenig im Weg stehen.

Als einleitendes Beispiel sehen Sie auf den Folien eine Funktion, die auf sehr komplizierte Art und Weise die Zahl 0 berechnet.
Immer.
Die gezeigte ~main()~-Funktion wird immer damit Enden, dass die Variable ~a~ den Wert 0 hat.
Prüfen Sie es nach, verfolgen Sie den Kontrollfluss, es wird immer 0 dabei herauskommen.
Wenn wir uns aber den Kontrollflussgraphen anschauen, den die Codeerzeugung produziert hat, so spiegelt sich darin genau die Struktur wieder, die der Entwickler notiert hat.

Geben wir dieses IR-Programm an den Optimierer, so wendet dieser eine Reihe von Optimierungsroutinen an, die zum Ziel haben, die Anzahl der ausgeführten IR-Befehle zu verringern.
Jede der im Beispiel aufgeführten Routinen werden wir in dieser Vorlesung besprechen und Sie finden im Übersetzer der Übung eine beispielhafte Implementierung.
Daher können Sie sich an dieser Stelle zurücklehnen und das Schauspiel genießen, wie Instruktionen sich verändern und verschwinden, und wie Blöcke und Kanten verschoben, verschmolzen und weggeräumt werden.
Wie wunderschön:
Ein Programm, das ein Programm verkürzt, ganz ohne menschliches Zutun und ohne dabei seine Semantik zu verändern.
Die Kontrollflussgraphen auf den Folien sind im übrigen echt und stammen aus dem Übungsübersetzer.

* Voraussetzungen und Ziele
:PROPERTIES:
:CUSTOM_ID: 09-goals-and-means
:END:

#+begin_src latex
  \dividerframe{Ziele und Voraussetzungen\\der Optimierung}

  \begin{frame}{Ziele bei IR-Level Optimierung}
    \begin{btBlock}[type=example]{Allgemeine Optimierungsziele}
      Ziel jeder Programm-Optimierung ist es, die \textbf{nicht-funktionalen} Eigenschaften, unter Beibehaltung der \textbf{erwarteten Funktionalität}, zu verbessern.
    \end{btBlock}

    \bi
    \ii Nicht-Funktionale Eigenschaften nach denen wir optimieren {
      \bi
      \ii \structure{Ausführungszeit}: Abbildung auf weniger oder schnellere Instruktionen
      \ii \structure{Programmgröße}: Verringerung der Binärgröße (eingebettete Systeme)
      \ii \structure{Energieverbrauch}: Verlängerung der Batterielaufzeit
      \ei
    }\medskip
    \ii<2-> Auf der Hardware-unabhängigen IR-Ebene fehlt das \alert{Kostenmodell}{
      \bi
      \ii Wir wissen nicht, wie schnell, groß oder Energie-hungrig einzelne Instruktionen auf der Zielplattform sein werden.
      \ii \structure{Heuristik}: \enquote{Speicherzugriffe und Sprünge sind teuer.}
      \medskip
      \ii<3->[$\Rightarrow$] Generelle Reduktion der Anzahl der Instruktionen und Basisblöcke
      \ei
    }
    \ei
  
  \end{frame}
#+end_src

Bevor wir zu einzelnen Optimierungsroutinen kommen, müssen wir genauer definieren, was die Ziele und Mittel sind, die ein Optimierer hat und die er einsetzen darf.
Das Ziel muss klar sein, genauso wie die Werkzeuge, die angewendet und die nicht angewendet werden dürfen.

Unser Startpunkt bei dieser Überlegung ist, wie immer im Übersetzerbau, der Entwickler und seine *Intention*. Verändern wir das Programm so, dass es nicht mehr dieser Intention entspricht, werden wir kein großes Glück in die Welt bringen. Die Art und Weise, wie diese Intention notiert wurde, ist der Programmtext, der nach den semantischen Regeln der Sprache zu Papier (oder eher zu Datei) gebracht wurde. Der Entwickler bedient sich der Regeln der Sprache und formuliert sein Programm entlang der Garantien, welche die Sprache bietet.

Eine Verletzung einer solche Regel würde Sie wirklich unglücklich machen. Ein Beispiel hierzu: Jede Programmiersprache verspricht, dass Sie jenen Wert aus einer Variable wieder auslesen können, den Sie bei der letzten Zuweisung hinein geschrieben haben. Würde beim Wiederauslesen ein anderer Wert herauskommen, wären Sie *maximal überrascht*[fn::Das Prinzip der *minimalen Überraschung* ist ein wichtiges Prinzip in der Informatik. Da in den abstrakt konstruierten Gedankengebäuden alles möglich ist, jedes Verhalten codierbar ist, ist es wichtig, sich diesem Prinzip unterzuordnen. Überraschen Sie weder Ihr zukünftiges Ich noch andere!]. Etwas formaler ausgedrückt würden wir sagen, dass ein Optimierer das Programm nur so verändern darf, dass seine erwartete Funktionalität, die von den Sprachregeln hoch und heilig versprochen wurden, unberührt bleiben.

Aber wozu das alles?
Was bedeutet es, ein Programm "besser" zu machen?
Um dies zu verstehen, müssen wir uns die *nicht-funktionalen Eigenschaften* des Programms anschauen.
Nicht-funktional bedeutet, dass diese Eigenschaften nicht Teil des Zweckes des Programm sind.
Selbst wenn die nicht-funktionalen Eigenschaften sehr sehr schlecht sind, könnten wir es benutzen, um einen gewissen Zweck zu verfolgen.
Jedoch würde es vielleicht sehr lange dauern[fn::Hunderttausend Jahre], viel Energie benötigen[fn::Mehrere Sonnen], oder viel Speicherplatz belegen[fn::Aller jemals hergestellte RAM].

Da ein Programm eine Vielzahl an nicht-funktionalen Eigenschaften hat, gibt es auch eine Vielzahl an unterschiedlichen *Optimierungszielen*.
Sie könnten sich vom Übersetzer wünschen, dass er das Programm dahingehend transformiert, dass es möglichst schnell ist (Ausführungszeit).
Sie könnten aber auch ein möglichst kleines Programm haben wollen, was gerade noch in Ihren Mikrocontroller passt (Programmgröße).
Manche dieser Optimierungsziele gehen Hand in Hand und weisen eine Korrelation auf.
So ist Energieverbrauch und Ausführungszeit häufig (aber nicht immer!) stark miteinander korreliert. Manchmal ist diese Korrelation aber auch invers, wie dies bei Programmgröße und Ausführungszeit der Fall sein kann.
Daher ist es wichtig, sich *vorher* zu überlegen, was die Eigenschaft ist, die man optimieren will.

Um bei dieser Verbesserung wirklich eine Verbesserung und keine Verschlimmbesserung durchzuführen, benötigt man eine Idee davon, welche statische Änderung am Programmtext welche dynamische Änderungen zur Laufzeit bewirkt:
"Sollte ich jene Instruktion durch diese andere Instruktion ersetzen, um das Programm schneller zu machen, oder benötigt die neue Instruktion mehr Takte?"
Um solche Fragen beantworten zu können, braucht man eigentlich ein detailliertes *Kostenmodell* für die konkrete Zielplattform.
Allerdings gibt es häufig ein solches Modell für den einen Prozessor, der in einer gewissen Generation aus einer bestimmten Herstellungscharge kommt, nicht.
Daher greifen Übersetzer normalerweise auf gröbere Heuristiken zurück, um Optimierungsentscheidungen zu treffen.
Beispielsweise sind solche Heuristiken, dass Speicherzugriffe zu vermeiden sind und das Sprünge negative Auswirkungen auf das Pipelining haben.

Für unser Vorhaben, das Optimieren auf IR-Ebene, gibt es noch ein weiteres Problem.
Denn wir wissen bei IR-Optimierungen ja gar nicht, für welche Prozessorarchitektur das Programm am Ende zu Assembler übersetzt wird.
Wir müssen also noch allgemeiner das Programm "besser" machen.
Daher werden wir versuchen,das Programm ganz generell kleiner zu machen (weniger Instruktionen, weniger Basisblöcke) und Instruktionen zu vermeiden, die in jedem Fall teurer in der Ausführung sind (weniger Sprünge).

#+begin_src latex
\begin{frame}{Wiederholung (1. Vorlesung): Korrekte Übersetzung}
    \bi
    \ii Die Programmiersprache definiert die \alert{beobachtbaren} Zustände. {
      \bi
      \ii Ein- und Ausgabe ist immer beobachtbar, oft aber auch Teile des Speichers
      \ii Bei C: Nur das Schreiben von globalen Variablen, nicht aber von lokalen
      \ei
    }\bigskip
    \ii Ein \advantage{korrekter} Übersetzer erhält alle beobachtbaren Zustände.{
      \bi
      \ii Parsing+Code-Erzeugung ist eine \textbf{Abbildungsfunktion}:\\[2ex]{
        \includegraphics[page=2,width=0.9\textwidth]{fig/01-zustand}
      }
      \ei
    }\bigskip
    \ii[$\Rightarrow$] Funktionen \advantage{dürfen verändert werden}, solange sie außen gleich aussehen. {
      \bi
      \ii Im IR-Code: lokale und temporäre Variablen sind nicht sichtbar
      \ii Von Außen sichtbar sind: \ircmd{Store}, \ircmd{Call}, \ircmd{Return}
      \ei
    }
    \ei
  \end{frame}
#+end_src  

In der erstem Vorlesung habe ich bereits darüber gesprochen, das eine korrekte Übersetzung die *beobachtbaren Zustände* eines Programms nicht verändern darf. Diese beobachtbaren Zustände sind genau jenes vom Entwickler erwartete Verhalten. Wie das Programm diese beobachtbaren Zustände generiert, ist aber im Grunde egal.

Für unser Vorhaben, die Optimierung auf IR-Ebene, gibt es erst mal nur zwei Dinge, die für ein Programm beobachtbar sind, weil sie Daten aus dem Programm an die Außenwelt transportieren können: Der Rückgabewert der ~main()~-Funktion und schreibende Speicherzugriffe.
Lassen wir es zusätzlich zu, dass unser Programm Ausgabe machen kann (indem es einen Systemaufruf absetzt), so wären auch diese beobachtbar.

Diese minimale Definition von beobachtbar (Rückgabewert, Schreibzugriffe, Systemaufrufe) ist allerdings nur dann von nutzen, wenn wir das gesamte Programm (== alle Funktionen) auf einmal optimieren.
Allerdings ist eine so weitreichende Optimierung meist viel zu umfassend, um handhabbar zu sein, weswegen wir die Optimierung einzelner Funktionen in Isolation anstreben.
In diesem Fall müssen wir die beobachtbaren Zustände strenger definieren:
Betrachten wir das Verhalten einer einzelnen Funktion, so können wir (von Außen), den Rückgabewert der Funktion (~Return~), die getätigten Schreibzugriffe (~Store~), und der Aufruf weiterer Funktionen (~Call~) beobachten.

Würden wir zwei Kontrollflüsse, einmal vom optimierten Programm und einmal vom Unoptimierten, aufzeichnen und nach diesen drei IR-Befehlen filtern, so müssten beide Sequenzen exakt gleich sein. Es dürften keine Umsortierungen stattgefunden haben und die  Operanden müssten genau übereinstimmen.

#+begin_src latex
  \begin{frame}{Phasen und Umfang der Optimierung}
    \begin{center}
      \includegraphics{fig/09-analyse}
    \end{center}
    \bi
    \ii Optimierung besteht immer aus \textbf{zwei Schritten} {
      \bi
      \ii \structure{Analyse} des Programms generiert Wissen, anhand dessen wir entscheiden können, ob wir \alert{eine Optimierung anwenden dürfen}.
      \ii \structure{Transformation} in ein semantisch äquivalentes, aber \enquote{besseres} Programm.
      \ei
    }\medskip
    \ii Optimierungen analysieren und transformieren \textbf{Programmausschnitte} {
      \bi
      \ii Wie groß ist der Ausschnitt, über den wir Wissen extrahieren?
      \ii Instruktionen, einzelne Basisblöcke, eine ganze Funktion, eine Aufrufhierarchie, mehrere interagierende Threads
      \ei
    }
    \ei
  \end{frame}
#+end_src

Im Allgemeinen zerfallen alle Optimierungsroutinen in zwei Schritte:
Bei der *Analyse* berechnet der Übersetzer alle für diese Optimierung relevanten Informationen aus dem Programm.
Aufgrund dieser Wissensbasis wird dann entschieden, ob die intendierte Optimierung auf den vorliegende Programmabschnitt überhaupt anwendbar ist, oder ob eine Optimierung das beobachtbare Verhalten verändern würde.
Ist die Entscheidung getroffen, dass optimiert werden soll, geht es zur *Transformation* des Programms. Dabei wird das IR-Programm, unter zur Hilfenahme der extrahierten Informationen, modifiziert.

Auf den Folien kann man beide Phasen deutlich getrennt voneinander sehen:
Wir schieben zwei einzelne Instruktionen in die Analyse, in der wir sehen, dass das ~IfGoto~ immer dasselbe tut und zu L0 springen wird (die Bedingung ist immer wahr).
Für das ~Add~ können keinen solchen Fakt generieren.
Aufgrund dieses Wissens tauscht die Transformation das ~IfGoto~ durch ein entsprechendes ~Goto~ aus, lässt aber das ~Add~ völlig unverändert.
Von den beobachtbaren Zuständen kann sich das Programm nicht geändert haben.

Wie groß die Abschnitte sind, die wir analysieren bzw. transformieren, hängt von der Optimierung ab.
Für manche Optimierungen genügt es, auf eine einzelne Instruktion zu blicken und genau diese Instruktion zu modifizieren.
Für andere Optimierungen müssen wir die ganze Funktion analysieren, um herauszufinden, dass wir einzelne Instruktionen verändern dürfen. Aber Transformationen können auch Basisblöcke und Kontrollflusskanten verändern.

* Optimierungen
In diesem Unterkapitel werden wir uns sechs verschiedene Optimierungsroutinen für den Zwischencode ansehen, den wir in der letzten Vorlesung definiert haben.
Dabei werden wir Techniken kennenlernen, deren grundlegenden Arbeitsweisen auch bei anderen Optimierungen zum Einsatz kommen.
Die besprochenen Themen sind so sortiert, dass wir von der Optimierung kleiner Einheiten (Instruktionen) zu den größeren Strukturen (Funktionen) im IR-Programm kommen.
Jedoch bleiben wir in dieser Vorlesung immer auf der Funktions-lokalen Ebene und betrachten keine Optimierungen, die über Funktionsgrenzen hinweg operieren.

** Instruktions- und Block-lokale Optimierungen
:PROPERTIES:
:CUSTOM_ID: 09-opts-instruction-local
:END:

#+begin_src latex
\dividerframe{Lokale Optimierungen}

  \begin{frame}{Algebraische Identitäten}
    \OrangeBox{Ersetzung von Instruktionen durch algebraisch äquivalente Instruktionen}

    \bi
    \ii<1-> \structure{Konstantenfaltung} ist eine Instruktions-lokale Optimierung\\[1ex]{
      \hfill\fbox{\texttt{a := \ircmd{Add} 1, 2}}\hfill$\Rightarrow$\hfill\fbox{\texttt{a := Assign 3}}\hfill\mbox{}\\[1ex]
      \bi
      \ii \textbf{Ziel}: Ersetzung von Instruktionen mit ausschließlich konstanten Operanden
      \ii Sprach-Semantik muss erhalten (32-Bit Programm vs. 64-Bit Übersetzer)
      \ei
    }\medskip
    \ii<2-> \structure{Strength-Reduction} verringert die Ausdrucksstärke einer Instruktion\\[1ex]{
      \hfill\fbox{\texttt{a := \ircmd{Mul} x, 2}}\hfill$\Rightarrow$\hfill\fbox{\texttt{a := Add x, x}}\hfill\mbox{}\\[1ex]
      \bi
      \ii Arithmetische Operationen haben unterschiedliche theoretische Komplexitäten
      \ii Multiplikation: $\mathcal{O}(n\cdot\log(n)\cdot\log(\log(n)))$ (FFT) vs. Addition: $\mathcal{O}(n)$
      \ii Ausdrucksstärke: Potenzierung $\gg$ Multiplikation $\gg$ Addition $\gg$ Bit-Shift
      \ei
    }\medskip
    \ii<3-> \structure{Technische Umsetzung}: Wende Ersetzungsregeln auf jede Instruktion an{
      \bi
      \ii \structure{Ersetzungsmuster} gibt Bedingungen vor, die Operanden erfüllen müssen
      \ii Ersetze die Instruktion im umgebenden Basisblock
      \ei
    }
    \ei
  \end{frame}

  \begin{frame}[fragile]{Algebraische Identitäten: Einige Ersetzungsmuster}
    \def\Var#1{\colorbox{safegreen!40}{#1}}
    \def\Const#1{\colorbox{badbee!40}{#1}}
    \def\Label#1{\colorbox{luhblue!40}{#1}}

    \centering
    \OrangeBox{Variablen: \Var{A}, Konstanten: \Const{x}, Labels: \Label{L}}\\[1ex]

    { \ttfamily
      \begin{tabular}{l|c|l}\toprule
        Orginal                                        & Bedingungen      & Ersetzung                              \\\midrule
        \multicolumn{3}{l}{\structure{Null-Operationen}}                                             \\
        \hspace{2em}\Var{A} := \ircmd{Add} \Var{B}, \Const{0}   & --               & \Var{A} := \ircmd{Assign} \Var{B}  \\
        \hspace{2em}\Var{A} := \ircmd{Mul} \Var{B}, \Const{1}   & --               & \Var{A} := \ircmd{Assign} \Var{B}  \\
        \hspace{2em}\Var{A} := \ircmd{Sub} \Var{B}, \Var{B}     & --               & \Var{A} := \ircmd{Assign} \Const{0}  \\\midrule
        \multicolumn{3}{l}{\structure{Konstantenfaltung}}                                             \\
        \hspace{2em}\Var{A} := \ircmd{Add} \Const{b}, \Const{c}    & --               & \Var{A} := \ircmd{Assign} \Const{b+c}  \\
        \hspace{2em}\ircmd{IfGoto} \Const{x}, \Label{A}, \Label{B} & \Const{x}!=0     & \ircmd{Goto} \Label{A}                 \\
        \hspace{2em}\ircmd{IfGoto} \Const{x}, \Label{A}, \Label{B} & \Const{x}==0     & \ircmd{Goto} \Label{B}                 \\\midrule
        \multicolumn{3}{l}{\structure{Strength-Reduction}}                                            \\
        \hspace{2em}\Var{A} := \ircmd{Pow} \Var{B}, \Const{2}      & --               & \Var{A} := Mul \Var{B}, \Var{B}        \\
        \hspace{2em}\Var{A} := \ircmd{Mul} \Var{B}, \Const{n}      & \Const{n}==$2^x$ & \Var{A} := LShift \Var{B}, \Const{x} \\
        \bottomrule
      \end{tabular}
    }
  \end{frame}
#+end_src

Die erste Optimierungsroutine analysiert einen IR-Befehl nach dem anderen und ersetzt diesen, falls es möglich ist, durch einen Befehl, der später effizienter ausgeführt werden kann.
Zwar haben wir auf der Ebene des IR-Codes kein Wissen darüber, wie lange die einzelnen Befehle schlussendlich tatsächlich brauchen werden, aber wir stützen uns auf Heuristiken und grobe Verhältnisse, die für viele Prozessorarchitekturen stimmen.
Zusätzlich ermöglichen diese Instruktions-lokalen Optimierungen weitere nachgelagerte Optimierungsschritte; dazu aber erst später mehr.

Die Kernidee dieser Familie von Optimierungen ist Instruktionen zu ersetzen durch Instruktionen, die *algebraisch Äquivalent* sind.
Dies bedeutet, dass die Ausführung der ursprünglichen Instruktion und die Ausführung der Ersetzung genau den gleichen nummerische Wert liefern.
Dazu betrachten wir nicht nur die Art der Instruktion, sondern auch die Werte der Operanden, insbesondere wenn diese *Konstanten* sind.

Bei der *Konstantenfaltung* sucht die Analyse nach Instruktionen, die ausschließlich konstante Quelloperanden haben und die keine Seiteneffekte, neben dem beschreiben ihres Zieloperanden, haben.
Insbesondere umfasst dies arithmetische, logische und bitweise Befehle, sowie bedingte Sprünge mit konstanter Sprungbedingung.
Da wir die Semantik eines jeden IR-Befehls genau definiert haben und die Quelloperanden konstant sind, können wir den Befehl bereits im Übersetzer ausführen.
Dieses Vorziehen der Ausführung ist deshalb möglich, weil der Befehl zur Laufzeit immer das selbe, konstante, Ergebnis liefern wird.
Unsere Analyse liefert uns also eine statische Voraussage über das dynamische Verhalten des Programms.
Die Transformation bei der Konstantenfaltung besteht dann darin, den Befehl durch eine einfache Zuweisung mit einem konstanten Operanden zu ersetzen.
Wichtig bei der Berechnung des Ergebnisses ist, dass wir darauf achten, die selbe Rechnung im Übersetzer zu durchzuführen, wie diese später, insbesondere an den Rändern des Wertebereichs, auch geschehen würde.

Möglich ist diese Ersetzung, weil wir eine algebraische Identität ausnützen: Der Ausdruck ~1+2~ und ~3~ sind algebraisch äquivalent, obwohl er strukturell unterschiedlich ist.
Diese Idee lässt sich teilweise auch dann anwenden, wenn nicht alle Argumente konstant sind.

Bei der *Strength-Reduction* ist das Ziel Operationen, die komplexitäts-theoretisch aufwendiger sind durch weniger komplexe Befehle zu ersetzen.
Informationstheoretisch ist beispielsweise eine Multiplikation teurer als eine Addition, die wiederum teurer ist als ein Bit-Shift.
Diese Hierarchie der arithmetischen Operationen können wir mit der Komplexitätstheorie begründen:
Bei der Addition wissen wir, dass eine Addition einer N-stelligen Binärzahl in N sequentiellen Bitmanipulationen (Volladdierer) durchgeführt werden kann; addieren also in O(N) geht.
Die Multiplikation dagegen ist deutlich aufwendiger und die effizienteren Multiplikationsalgorithmen{{{wikipedia_en(Schönhage–Strassen_algorithm)}}} haben eine höhere Schranke.
Dieser Unterschied in der Komplexität findet sich ebenfalls in der Hardware wieder; ein Addierwerk braucht deutlich weniger Gatter als eine Multiplikationsschaltung.
Daher lohnt es sich Multiplikationen durch Additionen, und Additionen durch Bit-Shifts zu ersetzen, wo auch immer dies möglich ist.

Konkret sucht die *Strength-Reduction* nach Operationen, bei denen einige der Quelloperanden konstant sind.
So ist die Multiplikation einer Variable mit der Zahl 2 äquivalent zur Addition mit Variable mit sich selbst. Noch einfacher wäre hier ein Links-Shift um eine Stelle, aber unser IR weißt leider kein ~LShift~ Kommando auf.
Häufig haben Strength-Reduction Optimierungen die Nebenbedingung, dass die Konstante eine Zweierpotenz ist oder ein andere zusätzliche Bedingung erfüllt.

Sehr ähnlich zur Strength-Reduction ist die Erkennung von *Null-Operationen*, bei denen die Quelloperanden unverändert durchgeschleift werden, oder sich vollständig auslöschen. So ist die Addition mit der Zahl 0 oder die Multiplikation mit der Zahl 1 eine Null-Operationen. Aber auch die Subtraktion einer Zahl mit sich selbst führt dazu, dass das Ergebnis im Vorhinein bekannt ist.

Technisch funktionieren diese Optimierungen mit algebraischen Identitäten so, dass die Analyse nach gewissen Mustern sucht (farbige Kästen auf der Folie), weitere Bedingungen prüft, und dann die Ersetzung durchführt.

Besonders hervorzuheben bei der Tabelle mit den Ersetzungsmustern ist das Muster für ~IfGoto~:
Wenn die Sprungbedingung eine Konstante ist, so können wir das ~IfGoto~ durch ein entsprechendes ~Goto~ ersetzen.
Spannend ist diese Ersetzung deshalb, weil diese Ersetzung als Seiteneffekt hat, dass sich der Kontrollflussgraph ändert, weil eine der ausgehenden Kanten des Basisblocks nicht mehr möglich ist.
Speichert die CFG-Datenstruktur die Nachfolger eines Blockes, und berechnet diese nicht jedes mal aus den Sprungbefehlen neu, so muss bei dieser Optimierung die Graphstruktur aktualisiert werden.

#+begin_src latex
  \begin{frame}{Konstanten- und Kopiepropagation}
    \begin{columns}
      \begin{column}{0.35\textwidth}
        \btAnimation[width=\textwidth]{range=1-3:<1->,3:<4->}{fig/09-cvp}
      \end{column}\hfill
      \begin{column}{0.60\textwidth}
        \begin{btBlock}{}\small
          Durch die einfache Codeerzeugung und bei der Optimierung entstehen konstante oder redundante Zuweisungen.
        \end{btBlock}
      \end{column}
    \end{columns}
    \medskip

    \bi
    \ii<handout:3-|3-> Propagation bekannter Werte durch einen Basisblock {
      \bi
      \ii Zum Zeitpunkt einer Zuweisung werden rechte und linke Seite äquivalent.
      \ii Operanden-Ersetzung in den folgenden Instruktionen
      \ii \ALERT{Aber}: Wir dürfen nur solange ersetzen, wie die Äquivalenz sicher ist.
      \ei
    }\medskip
    \ii<handout:4-|4-> \structure{Intuition} für die Propagation von Konstanten und Werten {
      \bi
      \ii Iteration über die Instruktionen eines BBs, Mitführen von Äquivalenzmengen
      \ii Zuweisungen etablieren eine neue Äquivalenz zwischen LHS und RHS
      \ii Andere Schreibzugriffe, die eine Variable verändern, löschen Äquivalenzen
      \ii Bei Lesezugriffen wird die Äquivalenzliste befragt und ggf. ersetzt.
      \ei
    }
    \ei
  \end{frame}

  \begin{frame}{Vorgehen für Wertpropagation}
    \btAnimation[width=0.75\textwidth]{center,range=4-11:<1->}{fig/09-cvp}

    \bi
    \ii \structure{Äquivalenzmengen} werden \enquote{durchgeschoben}, modifiziert und befragt {
      \be
      \ii<2-|handout:2-> Zu Beginn des Blocks ist nichts zu nichts anderem äquivalent
      \ii<3-|handout:3-> Bei Zuweisungen werden linke und rechte Seite äquivalent
      \ii<4-|handout:4-> Quelloperanden werden durch \advantage{äquivalente Konstanten oder Variablen} ersetzt
      \ii<4-|handout:4-> Äquivalenzmengen können mehr als 2 Elemente haben
      \ii<5-|handout:5-> Nicht-\ircmd{Assign} Schreiboperationen entfernen Elemente aus ihrer Menge
      \ee
    }
    \ei

    \OrangeBox<handout:8|8>{Kann die Funktion f, die Variable x, y, oder t0 verändern?}
  \end{frame}
 #+end_src

Durch die Ersetzung der Null-Operationen, und insbesondere durch die Konstantenfaltung, kommt es dazu, dass im Zwischencode viele Zuweisungen geschehen, die eine konstante rechte Seite haben.
Direkt nach der Zuweisung ist der Wert der Zielvariable also statisch bekannt.
Daher liegt der Gedanke nahe, die zukünftigen Verwendungen dieser Variable direkt durch die Konstante zu ersetzen.
So kann im Beispiel die Verwendung von ~t1~ durch die Konstante ~2~ ersetzt werden.


Genau das ist die Zielsetzung der *Konstantenpropagation*:
Wir möchten statisch bekannte Werte über Instruktionsgrenzen hinweg durch das IR-Programm durch propagieren.
Dabei müssen wir uns aber nicht nur auf konstante Werte beschränken, sondern wir können auch sogenannte "Kopien" mit der *Kopiepropagation* (Copy-Propagation) durchschleifen.
Zusammengefasst nennt man diese Art der Optimierung im Englischen daher Copy-Value-Propagation (CVP).

Bei der CVP gehen wir in Richtung des Kontrollflusses durch das Programm und ersetzen *Quelloperanden durch Konstanten oder Werte*, die an dieser Stelle garantiert den gleichen Wert haben.
Dabei müssen wir aber vorsichtig vorgehen, um nur Ersetzungen vorzunehmen, die immer und garantiert immer äquivalent sind.
Dazu führen wir während der Optimierung darüber Buch, welche Variablen und Konstanten vor jeder einzelnen Instruktion äquivalent sind.
Mit jeder Instruktion aktualisieren wir unsere Äquivalenzmengen und fahren mit der nachfolgenden Instruktion fort.
Bei dieser Optimierung können Analyse (Aktualisierung der Äquivalenzmengen) und Transformation (Ersetzung der Quelloperanden) verschränkt durchgeführt werden.

Zunächst schauen wir uns aber die CVP für einen einzelnen Basisblock an.
Später weiten wir diese Optimierung auf eine ganze Funktion aus, indem wir Äquivalenzmengen über Basisblöcke hinweg propagieren.
Aber zuerst müssen wir definieren, was eine *Äquivalenzmenge* ist:
Eine Äquivalenzmenge ist eine Menge von Variablen und Konstanten, die einen Gültigkeitszeitpunkt hat. Zu diesem Zeitpunkt haben alle Elemente den gleichen  R-Wert.
Da wir die Mengen anhand der Äquivalenz-Relation[fn::Die Äquivalenz-Relation ist transitiv!] bilden, gilt auch, dass jede Variable und jeder Wert zu einem Zeitpunkt nur in einer einzigen Äquivalenzmenge sein kann.
Anders ausgedrückt: Die Menge der zeitgleich gültigen Äquivalenzmengen *partitioniert* die Menge aller Variablen und Werte, wobei ein-elementige Äquivalenzmengen auftreten können (und auch die Regel sind).

Zum Zeitpunkt der Gültigkeit können wir einen Quelloperanden, wenn dieser in einer Äquivalenzmenge ist, durch jedes andere Element der Äquivalenzmenge ersetzen.
Bevorzugt werden wir natürlich Variablen durch Konstanten ersetzen, aber auch die Ersetzung von temporären Variablen durch benutzerdefinierte Variablen ist sinnvoll.
Es stellt sich nur noch die Frage:
Welche Regeln verwenden wir, um die Äquivalenzmengen zu erzeugen und zu aktualisieren?
Betrachten Sie sich hierzu die Folie.

Für einen einzelnen Basisblock starten wir ohne jegliches Wissen über irgendwelche Äquivalenzen.
Wir nehmen an, dass alle Variablen unbekannte Werte haben und keine Variable äquivalent zu einer anderen ist.
Durch diese konservative Annahme sind wir zum Beginn der Optimierung in einem sicheren Zustand.
Beim fortschreiten der CVP schieben wir die Menge der Äquivalenzmengen *vorwärts durch den Basisblock* und wenden verschiedene Aktualisierungsregeln abhängig von den IR-Befehlen an.


Die wichtigste Aktualisierungsregel für die CVP ist die für den ~Assign~-Befehl.
Denn mit einer Zuweisung entsteht (hinter der Zuweisung) eine Äquivalenz zwischen der rechten und der linken Seite.
Ist keine der beiden Seiten Teil einer größeren Äquivalenzmenge, so sind wir damit fertig, eine zwei-elementige Äquivalenzmenge aus linker und rechter Seite zu erzeugen.
Ist die rechte Seite bereits Element einer Äquivalenzmenge, so wird die linke Seite der Zuweisung dieser größeren Äquivalenzmenge zugeschlagen.

Besondere Vorsicht müssen wir nur walten lassen, wenn die linke Seite einer Zuweisung bereits Teil einer Äquivalenzmenge ist.
Da die linke Seite mit der Zuweisung überschrieben wird, müssen wir diese vorher aus ihrer Äquivalenzmenge löschen, bevor wir die neue Äquivalenz postulieren.
Dies gilt auch für alle anderen IR-Befehle, die eine Variable schreiben: Die Zielvariable muss aus allen Äquivalenzmengen entfernt werden (siehe ~x := Add x, y~), bevor neue Äquivalenzen etabliert werden.

Im Beispiel sehen wir auch, dass Ersetzungen und Modifikation der Äquivalenzmengen verschränkt durchgeführt werden (~y := Assign x~).
Für jeden einzelnen Befehl ist dabei die Reihenfolge:
(1) Ersetzungen durchführen, (2) Zieloperanden aus Äquivalenzmenge entfernen und (3) neue Äquivalenzen etablieren.
Dies entspricht der Reihenfolge in der Ausführung:
Zuerst werden Quelloperanden gelesen, bevor die Zieloperanden mit den berechneten Werten überschrieben werden.

Besonders spannend bei der Aktualisierung der Äquivalenzmengen sind solche Instruktionen, die einen Seiteneffekt haben.
Dazu zählt, insbesondere der ~Store~- und der ~Call~-Befehl.
Beide könnten direkt oder indirekt dazu führen, dass zwei Variablen nicht mehr äquivalent sind, obwohl sie nicht als Operand im IR-Programm vorkommen.
Dies werden wir uns nun genauer anschauen.

#+begin_src latex
   \begin{frame}{Alias-Problematik bei der Datenflussanalyse}

    \begin{center}
      \includegraphics[page=12]{fig/09-cvp}
    \end{center}


    \bi
    \ii Das Problem sind Zeiger, die unsere IR-Variablen \alert{verändern könnten}. {
      \bi
      \ii Wenn \texttt{ptr} auf \texttt{y} zeigt, kann sich \texttt{y} ändern, ohne explizit Operand zu sein.
      \ii Im Funktions-lokalen Scope wissen wir nicht, was \texttt{func()} mit \texttt{ptr} macht.
      \ii Die \ircmd{Store}-Instruktion könnte \texttt{y} auf 3 setzen.
      \ei
    }\medskip
    \ii<2-> Dies ist das \structure{Alias-Problem} für Zeiger. {
      \bi
      \ii Wir müssten genau wissen, auf welche Objekte ein Zeiger verweisen kann.
      \ii Dies erfordert aufwendige, teils inter-prozedurale, Analysen.
      \ii Beim geringsten Zweifel über die Alias-Relation \textbf{wirft der Optimierer hin}.
      \ei
    }\medskip
    \ii<3-> Einfache Heuristik für unsere Wertepropagation {
      \bi
      \ii Bei jedem \ircmd{Store} und bei jedem \ircmd{Call} werfen wir die Äquivalenzmengen weg. Better safe, than sorry.
      \ei
    }
    \ei
    \begin{overlaybox}<4|handout:2>[draw=srared,ultra thick,drop shadow,inner sep=1em]
      \columntitle{Take-Away für den effizienten Programmierer}\medskip

      \bii
      \ii Wildes Zeiger-rum-gefuchtel macht dem Übersetzer das Leben schwer.
      \ii Variablen sind umsonst, Wertepropagation eliminiert Zuweisungen.\\[1ex]
      \ii[$\Rightarrow$] Programmieren Sie \advantage{\textbf{verständlich}}, nicht \enquote{\ALERT{optimiert}}!
      \eii
    \end{overlaybox}
  \end{frame}
#+end_src

Das *größte Problem* für die CVP-Optimierung sind *Zeiger*.
Zeiger sind deswegen blöd, weil durch einen ~Store~-Befehl eine Zuweisung zu einer Variable erfolgen könnte, die im Befehl nicht als Operand vermerkt ist [fn::Was ja auch ziemlich genau der Zweck der Store-Instruktion ist: Variablen datenabhängig zu modifizieren.].
Da wir bei jeder Zuweisung und bei jedem Schreiben eines Zieloperanden die Äquivalenzmengen aktualisieren müssten, haben wir beim ~Store~ ein Problem.
Indirekt bekommen wir bei ~Call~ das gleiche Problem, da die aufgerufene Funktion ein solche Speichermanipulation durchführen könnte.

Nehmen wir mal an, dass der Zeiger ~ptr~ auf den Folien auf die Variable ~y~ zeigt.
In dem Fall müssten wir ~y~ aus der ~{y, 2}~-Menge entfernen und könnten eine ~{y, 3}~-Menge etablieren.
Allerdings müssten wir dazu *ganz sicher wissen*, dass ~ptr~ auf ~y~ zeigt.
Sind wir uns dessen nicht ganz sicher, ist das Aktualisieren der Äquivalenzmengen deutlich komplizierter.

Das Problem, was die CVP mit Zeigern hat, haben auch andere Optimierungsroutinen und wir nennen es das *Alias-Problem für Zeiger*.
Da Zeiger Werte sind, die durch das Programm hindurch propagieren, ist es schwierig bis unmöglich, festzustellen, auf welche Objekte ein Zeiger an einer gewissen Stelle zeigt/zeigen kann.
Im Allgemeinen ist das Alias-Problem sogar *unentscheidbar* und ein Übersetzer kann nur sichere Unterapproximation angeben, die durch aufwendige interprozedurale Analysen mühsam berechnet werden müssen.


In dieser Vorlesung lassen wir die Alias-Analyse weg und bedienen uns für die zwei kritischen Befehle bei der CVP einer sehr konservativen Heuristik:
Wir werfen bei jedem ~Store~ und bei jedem ~Call~ alle Äquivalenzmengen weg und starten wieder von vorne.
Die gesamte CVP-Analyse finden Sie im Übungsübersetzer.

Was wir aber aus der Kenntnis über das Alias-Problem lernen können, ohne eine Lösung dafür zu haben, ist, dass Zeiger für den Optimierer problematisch sind.
Insbesondere, wenn Sie wild mit Zeigern herumrechnen und kuriose Zeigermagie machen, wird der Optimierer aussteigen und Ihre Funktion teilweise oder gänzlich unoptimiert zurücklassen.
Wenn Sie also Gedacht haben "Ah, hier hab ich einen wirklich guten Trick mit Zeigern gefunden", so kann dies am Ende, für die Performance, gänzlich nach hinten losgehen.
Daher und auch aus einer Vielzahl an anderen Gründen:
Programmieren Sie verständlich und versuchen Sie nicht, im vorauseilendem Eifer, "optimierten Code" zu schreiben.

** Funktionsweite Optimierungen
:PROPERTIES:
:CUSTOM_ID: 09-opt-function-wide
:END:
#+begin_src latex
  \dividerframe{Funktionsweite Optimierungen}

  \begin{frame}{Wertepropagation über Basisblockgrenzen}
    \textbf{Bisher}: Wertepropagation (CVP) innerhalb eines Basisblocks

    \begin{center}
      \strut\alt<handout:1|1>{
        \siginline{CVP}{BasicBlock}{()}
      }{
        \siginline{CVP}{BasicBlock, Equivalences}{Equivalences}
      }
    \end{center}

    \bi
    \ii Wertepropagation ist eine \structure{Datenflussanalyse}{
      \bi
      \ii Wir verfolgen, wie Datenwerte entlang des Kontrollflusses \enquote{fließen}.
      \ii In einem Basisblock ist der Kontrollfluss determiniert linear.
      \ii Die CVP propagiert eine Äquivalenzmenge durch einen Block.
      \ei
    }\medskip
    \ii<handout:2-|2-> Wir wollen die CVP auf eine ganze Funktion anwenden. {
      \bi
      \ii Blöcke werden zu Transformator-Funktionen für Äquivalenzmengen.
      \ii Äquivalenzmengen \enquote{fließen} entlang der Kontrollflusskanten.
      \ii Fixpunkt-Iteration bis keine Änderungen mehr auftreten
      \ei
    }
    \ei
  \end{frame}
#+end_src

Bisher haben wir die CVP-Optimierung nur für einen einzelnen Basisblock durchgeführt.
Wir haben jeden Basisblock einer Funktion genommen und der CVP diesen Block, in Isolation, mit der Bitte um Verbesserung gegeben.
Dieses Vorgehen ist deswegen der Startpunkt der CVP, weil der Kontrollfluss innerhalb eines Basisblocks sehr einfach und genau definiert ist:
Von Oben, nach Unten.
Für jeden Befehl ist klar, welcher Befehl vorher und welcher Befehl hinterher ausgeführt wird.
Entlang dieses linearen Kontrollflusses ist ganz genau klar, wohin wir die Äquivalenzmengen propagieren müssen.
Wollen wir die Konstanten- und Kopiepropagation auf die Ebene einer ganzen Funktion heben, müssen wir aber auch damit umgehen können, wenn der CFG sich aufspaltet oder wieder zusammenkommt.

Grundlegend für die funktionsweite CVP-Optimierung ist, dass die Berechnung der Äquivalenzmengen eine *Datenflussanalyse* ist.
Der Datenfluss beschreibt, wie Werte zwischen Variablen und Operationen durch das Programm "fließen" und welche Werte mit welchen anderen Werten verrechnet werden.
Gewissermaßen gibt es neben dem Kontrollflussgraphen, der die Befehle einer Funktion ordnet, auch noch einen Datenflussgraphen, der die Variablen einer Funktion miteinander verbindet.
Datenfluss und Kontrollfluss sind eng aneinander gekoppelt und bedingen sich auch gegenseitig:
Der Kontrollfluss gibt vor, welche Instruktionen Operanden verrechnen und Bedingungen an Sprüngen bestimmen, wohin der Kontrollfluss geht.
Vereinigt sich der Kontrollflussgraph an einem Basisblock wieder, so fließen auch die Datenflüsse aus allen Vorgängerblöcken an dieser Stelle zusammen.

Um die CVP auf ganze Funktionen anzuwenden, verwenden wir das bisher gezeigte Verfahren für einen einzelnen Basisblock (~CVP(BasicBlock)~) als eine Transformationsfunktion für Mengen von Äquivalenzmengen (~Equivalences~).
Wir nehmen also eine Äquivalenzmengen-Menge und einen Basisblock und fragen:
Welche Äquivalenzen gelten nach diesem Basisblock, wenn dies die Äquivalenzen davor sind.
Bewaffnet mit dieser ~CVP(BasicBlock, Equivalences) -> Equivalences~ Funktion können wir Äquivalenzmengen entlang der Kontrollflusskanten propagieren lassen.
Wir führen die CVP für alle Blöcke dann solange aus, bis sich nichts mehr ändert und wir einen *Fixpunkt* erreicht haben.
Die Basisblock-übergreifende CVP wird also zu einer *Fixpunkt-Iteration*, die wir uns nun im genaueren anschauen werden.

#+begin_src latex
\begin{frame}{Fixpunktiteration auf einem CFG}

    \begin{columns}
      \begin{column}{0.49\textwidth}
        \btAnimation[width=\textwidth]{range=1-4:<1->, 4:<5->}{fig/09-fixpoint}
      \end{column}\hfill
      \begin{column}{0.49\textwidth}
        \begin{btBlock}{}
          \textbf{Problem}: Wie wenden wir eine Block-lokale Datenflussanalyse (z.B. CVP) auf eine ganze Funktion an?
        \end{btBlock}

        \medskip

        \columntitle{Elemente}
        \bii
        \ii<handout:2-|2-> Transformation-Funktion: \texttt{T()}
        \ii<handout:3-|3-> Propagierte Zustände: $d_n$
        \ii<handout:4-|4-> Zustands-Merge-Funktion: \texttt{M()}
        \eii

        \uncover<handout:5-|5->{
          \medskip
          \columntitle{Vorgehen}
          \bii
          \ii Initialisiere Zustände $d_n$
          \ii Iterierte Auswertung von \texttt{M(), T()}
          \ii Updaten der Block-Zustände
          \ii Stop, wenn Fixpunkt erreicht ist
          \eii
        }
      \end{column}
    \end{columns}
  \end{frame}
#+end_src

Um die CVP auf die Funktionsebene zu heben, schauen wir uns ganz generell an, wie Datenflussanalysen als Fixpunktiterationen auf dem Kontrollflussgraphen formuliert werden können.
Also, einen Schritt zurück:
Wir wollen eine Datenflussanalyse auf einer Funktion durchführen, die Wissen über den Zustand von Variablen berechnen soll.
Dieser Zustand wird durch IR-Befehle manipuliert.
Im Beispiel der CVP ist der Zustand, für den wir uns interessieren, die Menge der Äquivalenzmengen zu gewissen Zeitpunkten.

Für eine Fixpunktanalyse auf einem Kontrollflussgraphen brauchen wir drei Funktionen und einen Datentypen.
Der Datentyp ~d~ beschreibt den Zustand, für den wir uns interessieren.
Die erste Funktion initialisiert einen leeren Zustand, der minimal wenig Wissen beinhaltet.
Im CVP-Beispiel ist der minimale Zustand die leere Äquivalenzmengen-Menge.

Die erste "richtige" Funktion ist die Transformationsfunktion ~T()~, die einen Basisblock und einen Zustand als Parameter bekommt und einen neuen Zustand errechnet.
Die Transformationsfunktion fängt ein, welche Auswirkung ein Basisblock auf einen gegebenen Zustand hat.
Im CVP-Beispiel beginnt die Propagation der Mengen dann nicht mit der leeren Menge, sondern mit der gegebenen Menge.

Die zweite Funktion, die wir wiederholt aufrufen werden, ist die Zustands-Merge-Funktion ~M()~.
Diese Funktion hat mehrere Zustände als Parameter und berechnet die Verschmelzung dieser Eingangszustände.
Da jeder Eingangszustand eine gewisse Wissensbasis darstellt, verrechnet ~M()~ seine Eingabeparameter dahingehend, dass der Ausgangszustand das kombinierte Wissen darstellt.
Im CVP-Beispiel werden wir eine Äquivalenzmengen-Menge berechnen, die nur solche Variablen (und Konstanten) als äquivalent postuliert, die in allen Eingabezuständen äquivalent waren.
Keine Sorge, ich mache in einer Minute ein konkretes Beispiel hierzu.

Das Vorgehen der Fixpunktiteration ist dann wie folgt:
Wir legen auf jede Kante des Kontrollflussgraphen einen initialen leeren Zustand.
Wir nehmen uns dann, wiederholt, einen Basisblock B heraus und kombinieren alle Zustände, die auf eingehenden Kanten von B liegen mittels ~M()~.
Den kombinierten Zustand schieben wir durch ~T()~ und legen das Ergebnis auf die Ausgangskanten von B.
Verändert sich dadurch der Zustand auf einer Kante, so führen wir den Vorgang für diesen Nachfolgeblock noch einmal aus.
Diese Iteration führen wir solange durch, bis sich kein Zustand auf einer Kante mehr ändert.
Wir haben einen Fixpunkt erreicht.

#+begin_src latex
  \begin{frame}{Fixpunktiteration als Worklist-Algorithmus}
    \begin{code}[tag=Python]
      \lstinputlisting[style=py,style=smaller,linerange=s0-e0]{lst/09-fixpoint.py}
      \uncover<2->{\lstinputlisting[style=py,style=smaller,linerange=s1-e1]{lst/09-fixpoint.py}}
      \uncover<3->{\lstinputlisting[style=py,style=smaller,linerange=s2-e2]{lst/09-fixpoint.py}}
      \uncover<4->{\lstinputlisting[style=py,style=smaller,linerange=s3-e3]{lst/09-fixpoint.py}}
      \uncover<5->{\lstinputlisting[style=py,style=smaller,linerange=s4-e4]{lst/09-fixpoint.py}}
      \uncover<6->{\lstinputlisting[style=py,style=smaller,linerange=s5-e5]{lst/09-fixpoint.py}}
    \end{code}
  \end{frame}
#+end_src

Schauen wir uns den Code für einen solchen Fixpunktalgorithmus einmal genauer an.
Was Sie hier auf der Folie sehen, ist ein *Worklist-Algorithmus*, der parametrisiert verschiedene Datenflussanalysen durchführen kann.
Dieses Vorgehen heißt deswegen Worklist-Algorithmus, weil wir in einer Liste (~worklist~) speichern, für welche Blöcke wir das Verfahren noch einmal anwenden müssen.

Parametrisiert ist die Funktion ~fixpoint()~ mit den drei Funktionen (Initialisieren, Zustands-Merge und Transformation) sowie mit dem CFG, auf dem gearbeitet werden soll.

Als erstes legen wir uns ein Dictionary ~states~ an, um den Ausgangszustand für jeden Knoten des Graphen zu speichern.
Dies ist der Zustand, den ein Knoten ~node~ an seine Nachfolger propagiert. Wir initialisieren dieses Dictionary mit leeren Zuständen.

Der Algorithmus läuft dann solange bis ~worklist~ leer gelaufen ist.
In jedem Schleifendurchgang nehmen wir uns ein Element aus der Worklist und arbeiten dieses ab.
Zuerst werden alle Zustände der Vorgänger in ~d_ins~ gesammelt und mittels ~merge(d_ins)~ in ~d_in~ kombiniert.
Diesen kombinierten Zustand schieben wir durch ~transform(node, d_in)~.
Ist das Ergebnis der Transformation gleich dem bisherigen Ausgangszustand des Knotens, ist nichts zu tun.
Ansonsten setzen wir den Ausgangszustand neu und werfen alle Nachfolger in die Worklist, da deren Ausgangszustand sich ja dadurch ändern könnte, dass einer ihrer Eingänge sich geändert hat.

#+begin_src latex
\begin{frame}[t]{Fixpunktiteration als Datenflussanalyse}
    \bi
    \ii \structure{Voraussetzungen} für die Terminierung des Worklist-Algorithmus{
      \bi
      \ii Init+Merge+Transform muss Knotenzustände \alert{monoton} ändern
      \ii Beliebte Datenstrukturen für diesen Zustand: Bitvektoren, Mengen
      \ii Beispiel für eine monotone Zustands-Transformationen: {
        \bi
        \ii \texttt{init()}: \hspace{2cm}\btSetTab Generiere eine leere Bitvektoren
        \ii \texttt{merge()}: \btUseTab Bitweises ODER
        \ii \texttt{transform()}: \btUseTab Bits werden gesetzt
        \ei
      }
      \ei
    }\medskip
    \ii<2-> Verschiedene Geschmacksrichtungen {
      \bi
      \ii \structure{Vorwärts-Analyse}: Zustände werden in Richtung der CFG-Kanten propagiert
      \ii \structure{Rückwärts-Analyse}: Zustände werden entgegen der CFG-Kanten propagiert
      \ei
    }\medskip
    \ii<3-> \structure{Wertepropagation} als Fixpunktanalyse {
      \bi
      \ii Initialisierung mit leeren Äquivalenzmengen
      \ii Transformation wie gehabt mit \texttt{CVP(BasicBlock, Equivalences)}
      \ii Merge: \btSetTab Zwei Werte sind genau dann äquivalent,\\\btUseTab wenn sie in allen Eingabemenge äquivalent sind.
      \ei
    }
    \ei
  \end{frame}
#+end_src

Wollen wir einen solchen Fixpunktalgorithmus als Datenflussanalyse verwenden, so müssen wir darauf achten, dass dieses Verfahren auch wirklich terminiert.
Was würde passieren, wenn sich zwei Blöcke immer im Wechsel die Eingangszustände so manipulieren würden, dass der andere noch einmal in die Worklist kommt?
Um dies zu vermeiden, muss die Kombination aus den Zustandsinitialisierung, Zustands-Merge und Transformation *monoton* sein.
Dies bedeutet, dass jeder ausgeführte Schritt uns näher an den Fixpunkt bringt oder im aktuellen Zustand verweilt, niemals jedoch wieder weiter entfernt.
Auf diese Weise können wir garantieren, dass wir in endlich vielen Schritten einen Fixpunkt erreichen werden.

Ein Beispiel (nicht CVP) für eine solche monotone Kombination wäre:
Wir verwenden Bitvektoren als Zustände und starten mit gleich langen Bitvektoren, die an jeder Stelle 0 sind.
Jede Transformation kann einzelne Bits nur setzen, aber niemals löschen.
Die Vereinigung zweier Zustände ist das bitweise ODER (bei dem auch niemals eine 1 gelöscht wird).
Eine solche Datenflussanalyse wird immer einen Fixpunkt in einem Worklist-Algorithmus finden.

Wir haben bisher die Datenflussanalyse nur als Vorwärtsanalyse verwendet, da wir die Zustände nur entlang der Kontrollflusskanten propagiert haben.
Aber es gibt auch eine Rückwärtsanalyse, bei der man genau umgekehrt vorgeht und Zustände entgegen der Kontrollflusskanten propagiert.
Während man bei Vorwärtsanalyse gewissermaßen propagiert, was Basisblock an Daten erzeugt, kann man bei der Rückwärtsanalyse zurückpropagieren, was ein Basisblock benötigt.
Aber damit wollen wir uns hier nicht weiter beschäftigen.


#+begin_src latex
  \begin{frame}[fragile,t]{Beispiel für Wertepropagation}
    \vspace{-2em}
    \begin{columns}[t]
      \begin{column}{0.55\textwidth}
        \btAnimation[width=\textwidth]{raisebox,range=1-8:<1->}{fig/09-fixpoint-cvp}      
      \end{column}\hfill
      \begin{column}{0.44\textwidth}\mbox{}\\[3ex]
        \columntitle{Worklist}
        \centering
          \texttt{[%
            \only<handout:2|2>{BB0, BB1, BB3, BB2}%
            \only<handout:3|3>{BB1, BB3, BB2}%
            \only<handout:4|4>{BB3, BB2}%
            \only<handout:5|5>{BB2}%
            \only<handout:6|6>{BB3}%
            \only<handout:7|7>{BB0}%
            ]}\\[3ex]

        \uncover<handout:5-|5->{\columntitle{Merge Logbuch}}
        \def\L{\texttt\small}\small
        \def\E{$\emptyset$}
        \only<handout:5-|5->{\colorbox{badbee!20}{\medskip
        \begin{tabular}{lr}
          BB1 & \L{[\{a, c, 2\}, \{b, x, 3\}]}  \\
          BB2 & \L{\E}                          \\\midrule
          BB3 &\L{\E}                         
        \end{tabular}
      }}
      \mbox{}\bigskip\mbox{}
      \only<handout:7-|7->{\colorbox{badbee!20}{\medskip
        \begin{tabular}{lr}
          BB1 & \L{[\{a, c, 2\}, \{b, x, 3\}]}  \\
          BB2 & \L{[\{a, 2\}, \{b, x, 4\}]}     \\\midrule
          BB3 &\L{[\{a, 2\}, \{b, x\}]}         \\
        \end{tabular}
        }
      \bigskip\mbox{}
      Der Merge-Code ist fummelig.
    }
      \end{column}
    \end{columns}

  \end{frame}
#+end_src

Wenden wir das ganze doch an einmal auf die CVP-Analyse an, um es plastischer zu machen.
Auf der Folie sehen Sie die Basisblöcke (Blau) sowie Eingangs- und Ausgangszustand.
Ich zeige hier beide Zustände, um zu verdeutlichen, welchen Einfluss die Transformationsfunktion hat.
Im Konkreten muss man bei der CVP, die eine Vorwärtsanalyse ist, nur den Ausgangszustand speichern[fn::Bei einer Rückwärtsanalyse muss man den Eingangszustand speichern.], wie wir das auch im Codebeispiel getan haben.
Außerdem zeigt jede Folie den aktuellen Zustand der Worklist, alle noch abzuarbeitenden Knoten (rote Sternchen), sowie den aktuell bearbeiteten Knoten (roter Rahmen).

Wir beginnen damit alles zu initialisieren:
Alle Zustände sind zunächst leere Äquivalenzmengen-Mengen und alle Basisblöcke sollen wenigstens einmal bearbeitet werden und kommen daher in die Worklist.
Zuerst bearbeiten wir BB0, welcher zwei Äquivalenzen etabliert, die an die Nachfolger BB1 und BB2 propagiert werden. Wir fügen beide Nachfolger nicht noch einmal in die Worklist ein, da sie dort schon vertreten sind und ganz sicher noch einmal bearbeitet werden.

Der Knoten BB1 hat nur einen Vorgänger, was den Zustands-Merge trivial macht.
Die beiden Instruktionen in BB1 werden beim Transformieren modifiziert und der Ausgabezustand enthält mehr Äquivalenzen als der Eingabezustand. BB3 würde in die Liste kommen, ist dort aber bereits vorhanden.

Als nächsten bearbeiten wir (aus didaktischen Gründen) BB3, da die Reihenfolge in der Worklist nicht festgelegt ist.
BB3 hat jetzt zwei Vorgänger, sodass wir deren Zustände vereinigen müssen.
Für BB1 ist der Zustand schon durch Transformation entstanden, bei BB2 ist es noch der initiale Zustand.
Für die CVP müssen wir beim Zustands-Merge wie folgt vorgehen:
Im kombinierten Zustand dürfen zwei Werte (Variablen oder Konstanten) nur genau dann in einer Äquivalenzmenge sein, wenn diese in allen eingehenden Zuständen auch äquivalent sind.
Etwas veranschaulicht gesprochen:
Wenn die Variable A im einen Vorgänger den Wert 2 erhalten hat, im anderen Vorgänger aber den Wert 3, können wir nichts mehr über den Wert der Variable sagen.
Im Falle von BB3 kombinieren wir etwas mit der leeren Äquivalenzmengen-Menge, was zwingend auch die leere Äquivalenzmengen-Menge sein muss.

Jetzt führen wir endlich BB2 aus, was wiederum Äquivalenz erzeugt und BB3 wieder in die Worklist katapultiert. 

Beim zweiten Besuch von BB3 sieht die Situation vom Mergen schon ganz anders aus:
Die Äquivalenz ~{a, 2}~ hat sich über beide Pfade der Fallunterscheidung hindurch propagiert und bleibt für BB3 erhalten.
Zusätzlich kommt hinzu, dass ~b~ und ~x~ in beiden Pfaden so gesetzt werden, dass sie den gleichen Wert haben:
Linksherum haben beide den Wert 3, rechtsherum haben beide den Wert 4.
In jedem Fall haben beide Variablen aber den gleichen R-Wert.

Jetzt wird BB0 noch einmal ausgeführt, aber da wir eine leere Menge in den Eingangsblock propagieren, ändert sich hier nichts mehr und wir sind fertig und haben einen Fixpunkt erreicht.

Sagen möchte ich noch, dass der Code zum Äquivalenzmengen-Mengen Vereinigen etwas fummelig ist, weswegen ich das Vorgehen hier nicht genauer darlegen möchte. Die Intuition ist, das man gemeinsamen Teilmengen für Äquivalenzmengen aus unterschiedlichen Vorgängern bildet, die sich teilweise überschneiden. Das ganze geht sogar ziemlich effizient, wenn man eine [[https://de.wikipedia.org/wiki/Union-Find-Struktur][Union-Find Datenstruktur]] verwendet. Und weil ich diese Union-Find Strukturen so schön finde, möchte ich hier auch eine Lanze dafür brechen: Schauen Sie sich diese Datenstruktur an, sie ist wirklich wundervoll elegant.

** Optimierung des Kontrollflusses
:PROPERTIES:
:CUSTOM_ID: 09-opt-cfg
:END:
#+begin_src latex

  \dividerframe{Optimierung des Kontrollflusses}

  \begin{frame}{Verschmelzen von konsekutiven Basisblöcken}
    \begin{columns}
      \begin{column}{0.4\textwidth}
        \btAnimation[width=\textwidth]{range=1-2:<1->,3:<3->}{fig/09-block-merge}
      \end{column}\hfill
      \begin{column}{0.6\textwidth}
        \bii
        \ii \textbf{Problem}: Sprung zu BB1 ist überflüssig{
          \bi
          \ii BB0 hat genau einen Nachfolger
          \ii BB1 hat genau einen Vorgänger
          \ii[$\Rightarrow$] Verschmelze BB0 und BB1
          \ei
        }\medskip
        \ii<handout:2-|2-> \textbf{Analyse}: Finde Block-Paar (x, y), sodass...{
          \bi
          \ii y ist Nachfolger von x
          \ii x hat genau einen Nachfolger
          \ii y hat genau einen Vorgänger
          \ei
        }
        \ii<handout:3-|3-> \textbf{Transformation}: Verschmelze $x\rightarrow y${
          \bi
          \ii Lösche \ircmd{Goto} aus x
          \ii Hänge Instruktionen von y an x
          \ii Block y wird später gelöscht
          \ei
        }
        \eii
        \begin{btBlock}<handout:4-|4->[type=info]{Trennung der Belange}\small
          Das Aufräumen des toten Blocks (BB1) übernimmt eine andere Optimierung.
        \end{btBlock}
      \end{column}
    \end{columns}

  \end{frame}
#+end_src

Nachdem wir uns mit der CVP-Optimierung ziemlich lange aufgehalten haben, kommen jetzt einige deutlich einfacher zu verstehende Optimierungen.
Diese Arbeiten auf der Ebene des Kontrollflussgraphen und haben zum Ziel, unnötige Sprünge und Basisblöcke zu entfernen. Lehnen Sie sich also zurück und genießen Sie, wie Kanten umgebogen werden und Blöcke verschwinden.

Als erstes wollen wir Basisblöcke *verschmelzen*, die eh immer direkt hintereinander ausgeführt werden.
Der Sprung vom ersten zum zweiten Block ist also absolut überflüssig. Als zusätzliche Bedingung machen wir aber, dass der zweite Block auch wirklich nur vom ersten Block aus angesprungen werden kann. Der erste Block soll also nur einen Nachfolger und der zweite nur einen Vorgänger haben.

Als Transformation löschen wir das ~Goto~ aus dem ersten Block und kopieren alle Instruktionen in den ersten Block hinein. Dies hat zur Folge, dass der erste Block nicht mehr den zweiten Block als Nachfolger hat, sondern die Nachfolger seines Nachfolgers. 

Der zweite Block kann nun auch niemals mehr angesprungen werden und ist daher *Tod*.
Wir räumen diesen aber nicht an dieser Stelle weg, sonder überlassen das Wegräumen von toten Blöcken einer anderen Optimierung, die sich dann auch darum kümmert, tote Basisblöcke zu entfernen, die durch Konstantenfaltung von ~IfGoto~ Befehlen entstehen können.

#+begin_src latex
  \begin{frame}{Elimination von unbedingten Doppelsprüngen}

      \begin{columns}
      \begin{column}{0.4\textwidth}
        \btAnimation[width=\textwidth]{1:<1-2>, 2:<3>, 3:<4->}{fig/09-redundant-jump}
      \end{column}\hfill
      \begin{column}{0.6\textwidth}
        \bii
        \ii \textbf{Problem}: BB2 ist überflüssig{
          \bi
          \ii BB2 enthält \textbf{nur} ein \ircmd{Goto}
          \ii Jeder Sprung zu BB2 ist ein Sprung zu BB3
          \ii[$\Rightarrow$] \textbf{Ziel}: Vermeide redundante Sprünge
          \ei
        }\medskip
        \ii<handout:2-|2-> \textbf{Hinweis}: Verschmelzen hier unmöglich {
          \bi
          \ii BB2 hat mehrere Vorgänger
          \ii Verschmelzen würde BB-Bedingung verletzen
          \ei
        }\medskip
        \ii<handout:3-|3-> \textbf{Analyse}: Finde Block x sodass\ldots{
          \bi
          \ii x nur ein \ircmd{Goto} enthält
          \ii x hat y als einzigen Nachfolger
          \ei
        }\medskip
        \ii<handout:4-|4-> \textbf{Transformation}:{
          \bi
          \ii Besuche alle Vorgänger von x
          \ii Ersetze jedes x.label durch y.label
          \ii Block x wird später gelöscht
          \ei
        }
        \eii
      \end{column}
    \end{columns}

  
  \end{frame}

#+end_src

Die zweite CFG-Optimierung soll Sprünge entfernen, die nur auf Sprünge springen: Doppelsprünge.
Unsere Codeerzeugung{{{see(08-codegen-control-flow,Codeerzeugung für Kontrollflusskonstrukte)}}} provoziert solche Situationen, da sie für jede Bedingung einen Else-Block anlegt, auch wenn es auf Quellcodeebene keinen Else-Zweig gab.
Diese Sprünge sind offensichtlich unnötig, da sie immer das gleiche machen. In gewisser Weise sind diese Situationen auch konstante Operationen, allerdings auf der Ebene des Kontrollflusses und nicht, wie bei der Konstantenfaltung, auf der Ebene des Datenflusses.

Bemerkenswert ist, dass solche Doppelsprünge nicht in allen Fällen durch die bereits diskutierte Blockverschmelzung abgedeckt sind.
Besehen Sie sich das Beispiel und BB2:
In Rückrichtung können wir BB2 nicht verschmelzen, da es mehrere Vorgänger hat.
In Vorwärtsrichtung können wir BB2 nicht mit BB3 verschmelzen, da dieser selbst auch mehrere Vorgänger hat.
Zwar würde das Verschmelzen in beiden Fällen kein Problem darstellen, da BB2 selbst keine Instruktionen hat, aber wir müssten dafür bei der Verschmelzungsoptimierung einen Sonderfall für leere Blöcke einbauen.
Besser ist es, eine separate Optimierungsroutine zu bauen, die spezifisch auf Doppelsprünge abstellt.

Die *Analyse* für diese Optimierung ist relativ einfach:
Wir suchen alle Basisblöcke ~x~, die genau ein ~Goto~ enthalten.
Damit ist bereits sichergestellt, dass der Block nur einen Nachfolger (~y~) hat.

Die *Transformation* sieht dann so aus, dass wir in der ganzen Funktion nach Labeln suchen, die auf den Block ~x~ zeigen.
Egal ob diese in einem ~Goto~ oder einem ~IfGoto~ vorkommen.
Einschränken können wir diese Suche, indem wir im Kontrollflussgraphen rückwärts zu den Vorgängern von ~x~ gehen.
In den gefunden Sprüngen ersetzen wir das ~x~-Label durch das ~y~-Label.
Wiederum überlassen wir das Aufräumen des toten Basisblocks der nun folgenden Dead-Block-Elimination.

#+begin_src latex
  \dividerframe{Aufräumen}

  \begin{frame}{Elimination von toten Basisblöcken}
    \begin{columns}
      \begin{column}{0.4\textwidth}
        \btAnimation[width=0.75\textwidth]{center,padding=1em,range=4-6:<1->}{fig/09-block-merge}
        \btAnimation[width=0.75\textwidth]{center,padding=1em,range=4-6:<1->}{fig/09-redundant-jump}
      \end{column}\hfill
      \begin{column}{0.58\textwidth}
        \bii
        \ii \textbf{Problem}: Optimierungen hinterlassen Müll{
          \bi
          \ii Aufräumen von totem Code zentralisieren
          \ii Blöcke, die nie angesprungen werden
          \ei
        }\medskip
        \ii<handout:2-|2-> \textbf{Analyse}: Finde tote Blöcke{
          \bi
          \ii Blöcke ohne Vorgänger
          \ii Keine Label-Adresse im Umlauf
          \ei
        }\medskip
        \ii<handout:2-|2-> \textbf{Transformation}: Lösche Block {
          \bi
          \ii Entfernung aller ausgehenden Kanten
          \ii Block bei umgebender Funktion entfernen
          \ei
        }
        \eii
      \end{column}
    \end{columns}

  \end{frame}
  #+end_src

Die restlichen beiden Optimierungen beschäftigen sich mit dem Aufräumen von überflüssigen Programmelementen.
Zuerst wollen wir, mittels der *Dead-Block-Elimination*, tote Basisblöcke entfernen.

Ein Basisblock ist tot, wenn es keine Möglichkeit mehr gibt ihn zu betreten, er also in keinem Kontrollfluss vorkommen kann.
Ganz im Allgemeinen ist die Frage, ob ein Basisblock wirklich tot ist, gleichbedeutend mit dem *Halteproblem*.
Stellen Sie sich ein if-Statement mit sehr komplexer Bedingung vor, die von Nutzereingaben abhängig ist.
Es ist für den Übersetzer schwierig bis unmöglich, herauszufinden, ob diese Bedingung jemals wahr wird und der bedingte Block jemals ausgeführt wird oder ob dieser tot ist.
Wir begnügen uns bei der Dead-Code-Elimination daher mit einer *sicheren Unterapproximation*:
Nur wenn wir garantieren können, dass der Block tot ist, entfernen wir ihn.

Unsere Unterapproximation ist denkbar einfach:
Wird ein Block niemals angesprungen und ist er nicht der Eingangsblock der Funktion, so ist er sicher tot und kann entfernt werden.
Wir suchen also in der *Analyse* nach Blöcken, die keinen Vorgänger im CFG haben und nicht der Eingangsblock sind.

Zusätzlich müssen wir darauf achten, dass es keinen Zeiger gibt, der auf diesen Basisblock zeigt.
Zwar ist es in unserer IR-Maschine unmöglich, Referenzen auf Label zu erzeugen, aber das gilt im Allgemeinen nicht für alle IR-Sprachen.
Erlaubt eine IR-Maschine ~ptr := Reference ..BB3~ zu schreiben und Variablen anstatt von statischen Labeln zu verwenden (~Goto ptr~), so müssen wir noch vorsichtiger sein. In diesem Fall würden wir alle Blöcke, für die eine Referenz erzeugt wurde, auch als *nicht-tot* markieren.

Die *Transformation* ist dann relativ einfach: Wir entfernen den toten Block und alle CFG-Kanten, die von ihm ausgehen, aus der Funktion. 

Bemerkenswert ist, dass das Entfernen eines toten Blockes dazu führen kann, dass andere Optimierungen wieder möglich werden.
So verlangt Verschmelzung, dass der zweite Block nur einen Vorgänger hat.
Hat der zweite Block allerdings tote Vorgänger, so kann deren Entfernung eine Verschmelzung ermöglichen.


** Elimination toter Instruktionen
:PROPERTIES:
:CUSTOM_ID: 09-opt-dead-instruction
:END:
#+begin_src latex
  \begin{frame}[t]{Elimination von toten Variablen}
    \begin{center}
      \vspace{-2em}
      \btAnimation[width=0.5\textwidth]{range=1-9:<1->}{fig/09-dve}  
    \end{center}

    \bi
    \ii \textbf{Problem}: Vorangegangene Optimierungen machen Variablen überflüssig{%
      \bi
      \ii \structure{Konstantenfaltung} ersetzt Operationen durch ihr konstantes Ergebnis
      \ii Durch \structure{Wertepropagation} werden äquivalente Quelloperanden vereinigt
      \ei
    }
    \ii<handout:8-|8-> \textbf{Analyse}: Finde alle lokalen Variablen, die \advantage{nie gelesen werden}{
      \bi
      \ii \alert{Vorsicht}: Zeiger können Variablen indirekt lesen (Alias-Problem)
      \ii \advantage{Heuristik}: Markiere alle Variablen, deren Adresse berechnet wird, als \enquote{gelesen}
      \ei
    }
    \ii<handout:9-|9-> \textbf{Transformation}: Entferne niemals gelesene lokale Variable x {
      \bi
      \ii Lösche Instruktionen, die x schreiben und kein \ircmd{Call} sind (Seiteneffekte!)
      \ii Lösche x aus der umgebenden Funktion
      \ei
    }
    \ei
  
  \end{frame}

#+end_src

Neben den toten Blöcken hinterlassen manche Optimierungen auch *tote Instruktionen*.
Eine Instruktion ist dann tot, wenn ihr Ergebnis oder ihre Seiteneffekte nicht benötigt werden.
Insbesondere die CVP-Optimierung in Verbindung mit der Konstantenfaltung führt dazu, dass Ergebnisse vorberechnet und durch das Programm propagiert werden.
Hierdurch verbleiben in den Basisblöcken viele IR-Befehle, die einer Variable einen Wert zuweisen, der niemals ausgelesen wird. Wir können uns diese Zuweisungen also auch getrost sparen.

Wie bei der Suche nach toten Blöcken müssen wir uns einer Heuristik bedienen, um tote Instruktionen zu finden.
Wir ziehen das Ganze daher von Seiten der Variablen auf und suchen nach zunächst nach toten Variablen, die niemals gelesen werden und auch niemals gelesen werden können.

In der *Analyse* finden wir alle Variablen, die an keiner Stelle in der Funktion als Quelloperand auftreten und daher nie gelesen werden.
Insbesondere ist darauf zu achten, dass ~Reference var~ eine Variable als gelesen markiert, da der erzeugte Zeiger dazu führen könnte, dass eine Variable unbemerkt gelesen wird. Wieder kommt das Alias-Problem zum Vorschein.

Die *Transformation* entfernt dann alle Instruktionen, die eine tote Variable als Zieloperanden haben.
Ausgenommen hier von sind ~Call~-Instruktionen, da diese einen Seiteneffekt haben, den wir auf der funktionslokalen Ebene nicht überblicken können.
Nachdem die toten Instruktionen entfernt wurden, können auch die toten Variablen aus der IR-Funktion gelöscht werden.

Zusammen fallen die Entfernung toter Blöcke und die Entfernung toter Instruktionen in den Bereich der *Dead-Code-Elimination*.
In dieser Klasse von Optimierungen werden Programmteile entfernt, die niemals ausgeführt werden oder deren Ergebnis eh ignoriert wird.
In dieser Klasse gibt es noch präzisere Optimierungen, die ich hier aber aufgrund der Kürze der Zeit nicht erklären kann.
Jedoch können Sie einmal für sich überlegen, wie eine Datenflussanalyse in Rückrichtung arbeiten würde, die Werte erkennt, die nicht in den Return-Wert einfließen.

* Optimierung im größeren Zusammenhang
:PROPERTIES:
:CUSTOM_ID: 09-opt-big-picture
:END:

#+begin_src latex
  \dividerframe{Optimierungen\\im\\Kontext}
  \begin{frame}[fragile]{Optimieren einer Funktion}
    \begin{columns}
      \def\BOOL{\bgroup\uncover<handout:2|2>{\color{luhblue}bool\space}\egroup}
      \begin{column}{0.49\textwidth}
        \begin{code}\ttfamily\scriptsize
        \BOOL ConstantFolding(Function);\\
        \BOOL ValuePropagation(Function);\\
        \BOOL MergeBasicBlocks(Function);
      \end{code}

      \end{column}\hfill
      \begin{column}{0.49\textwidth}
        \begin{code}\ttfamily\scriptsize
        \BOOL EliminateRedundantGotos(Function);\\
        \BOOL EliminateDeadBlocks(Function);\\
        \BOOL EliminateDeadVariables(Function);
      \end{code}
      \end{column}
    \end{columns}

    \medskip
    \bi
    \ii Wir haben sechs \structure{Optimierungsverfahren für  Zwischencode} kennengelernt{
      \bi
      \ii Optimierungen ermöglichen sich gegenseitig Optimierungen
      \ii In welcher Reihenfolge sollen wir die Optimierungen anwenden?
      \ei
    }\medskip
    \ii<handout:2-|2-> Optimierung als Fixpunktiteration {
      \bi
      \ii Jedes Verfahren gibt zurück, ob die Funktion \alert{verändert} wurde
      \ii Bei Veränderung führen wir alle anderen Optimierungen \textbf{nochmals aus}
      \ei
    }
    \ei

    \begin{columns}
      \begin{column}{0.49\textwidth}\centering
        \includegraphics[width=3.5cm] {fig/09-optimization-depends}
      \end{column}
      \begin{column}<handout:2-|2->{0.49\textwidth}
        \begin{code}[]
          \begin{py}
            def optimize(func):
              changed = True
              while changed:
                changed = False

                for optimizer in optimizers:
                  ret = optimizer(func)
                  changed = changed or ret 
          \end{py}
        \end{code}
      \end{column}
    \end{columns}
  \end{frame}


  \begin{frame}{Meilensteine der optimierenden Übersetzer}
    Optimierende Übersetzer sind seit vielen Jahrzehnten ein aktives Forschungsebiet.
  Hier einige Arbeiten, die \textbf{ich} für Meilensteine halte:\\[1em]
  
    \begin{description}
      \item[1973] Datenflussanalyse als Fixpunktiteration    \hfill Kildall
      \item[1977] Abstrakte Interpretation                   \hfill Cousot und Cousot
      \item[1979] Portable Peephole Optimizer                \hfill Fraser
      \item[1989] Single-Static Assignment wird effizient    \hfill Cytron
      \item[1993] Verschmelzen von Schleifen                 \hfill Kelly and Pugh
      \item[1996] Effiziente Interprozedurale Alias Analyse  \hfill Steensgaard
      \item[2004] Low-Level Virtual Machine (LLVM)           \hfill Lattner
      \item[2010] Polyhedral Loop Optimization in GCC        \hfill Trifunovic
    \end{description}
  \end{frame}
#+end_src

In dieser Vorlesung haben wir sechs verschiedene Optimierungsroutinen kennengelernt, die alle auf der Granularität von einzelnen Funktionen anwendbar sind.
Allerdings haben wir noch nicht darüber gesprochen, wie diese Optimierungsroutinen im Übersetzer angeordnet sind und ausgeführt werden.
Eine Möglichkeit ist, die Optimierungen zu sortieren und diese Sequenz einmal abzuarbeiten.
Allerdings würde sich bei diesem Vorgehen die Frage stellen, welche Reihenfolge das beste Optimierungsergebnis liefert.
Außerdem haben wir gesehen, dass manche Optimierungen andere Optimierungen erst möglich machen.
Zeichnet man ein Diagramm, bei dem man mit Pfeilen ausdrückt, welche Optimierung welche andere Möglich machen könnte (siehe Folie), so kommt man schnell dahin zu Erkennen, dass es hier zyklische Abhängigkeiten zwischen den Optimierungen gibt.
Dies bedeutet, dass keine lineare Optimierungssequenz jemals das bestmögliche Ergebnis liefern kann; Optimierungen sollten also mehrfach ausgeführt werden.

Ein Möglichkeit die einzelnen Optimierungsroutinen zu komponieren ist es, eine Fixpunkt-Iteration zu unternehmen.
Dazu bekommt jede Routine einen Rückgabewert, der anzeigt, ob sie eine Änderung an der IR-Funktion vorgenommen hat.
Nun führen wir alle Optimierungen solange aus, bis keine Routine mehr eine Änderung vorgenommen hat.
Natürlich gibt es noch klügere Methoden diese Komposition vorzunehmen, zum Beispiel indem man den Abhängigkeitsgraphen beachtet, aber für einen möglichst einfachen Übersetzer ist die naive Fixpunktiteration eine gute Möglichkeit, die Optimierungsroutinen wiederholt auf einer Funktion auszuführen.

Damit endet unsere Betrachtung der Optimierungsphase in Übersetzern.
Ich hoffe Ihnen zumindest einen Einblick darin gegeben zu haben, welches Potential Optimierungen haben und wie stark sie die Effizient von Programmen steigern können.
Haben Sie vertrauen darin, dass Ihr Übersetzer Optimierungen durchführt und schreiben Sie lieber Code, der verständlich ist und halten Sie Abstand von Code, der "Optimierung" durch übermäßige Komplexität vortäuscht. Wildes rumfuchteln mit Zeigern mag beeindruckend aussehen, aber die dadurch entstehende Verschlimmerung des Alias-Problems kann in der Gesamtbilanz sogar negativ ausfallen.

Auf der letzten Folie habe ich noch einige Meilensteine der optimierenden Übersetzer aufgelistet.
Insbesondere sollte Ihnen dabei auffallen, dass dieses Gebiet bereits sehr lange aktiv beforscht wird, aber auch dass die Forschung daran keineswegs schon abgeschlossen ist.
So ist die hier länglich diskutierte Datenflussanalyse schon 1973 vorgestellt worden, jedoch habe ich mich beim Design des Zwischencode von LLVM inspirieren lassen.
Aufwendigere Programmoptimierungen erfordern sowohl ein solides mathematisches Modell, um korrekte Analysen durchzuführen, als auch ein gutes Verständnis von Hardware und Systemsoftware, um Optimierungspotentiale richtig zu bewerten.

* Zusammenfassung
#+begin_src latex
  \begin{frame}{Zusammenfassung}
    \bi
    \ii Optimierungen nur von \structure{nicht-funktionalen Programmeigenschaften} {
      \bi
      \ii Aus Sicht der Sprache bleibt das \structure{beobachtbare Verhalten} gleich.
      \ii Verschiedene Optimierungsziele, wobei es meist um \structure{Ausführungszeit} geht
      \ei
    }\medskip
    \ii Optimierungen sind immer zweigeteilte Übersetzungsschritte. {
      \bi
      \ii \structure{Analyse} des Programms berechnet Informationen, die wir brauchen, um zu entscheiden, \textbf{ob und wie} eine Optimierung angewendet wird.
      \ii \structure{Transformation} der Zwischenrepräsentation des Programms setzt die Optimierung durch.
      \ei
    }\medskip
    \ii Auf \structure{IR-Ebene} reduzieren wir die Anzahl der Instruktionen und Sprünge{
      \bi
      \ii \structure{Konstantenfaltung} ersetzt konstante Berechnungen durch ihr Ergebnis
      \ii \structure{Wertepropagation} ersetzt Operanden durch äquivalente Werte/Konstanten
      \ii \structure{Verschmelzen} von streng konsekutiv ausgeführte Blöcke
      \ii \structure{Redundante Doppelsprünge} werden aufgelöst
      \ii \structure{Tote Blöcke} und \structure{tote Variablen} werden entfernt
      \ei
    }
    \ei
  
  \end{frame}
#+end_src
